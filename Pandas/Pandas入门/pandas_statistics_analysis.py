#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Pandasç»Ÿè®¡åˆ†æå®Œå…¨æ¼”ç¤º
åŸºäºã€ŠPandasé«˜çº§æ“ä½œå®Œå…¨æŒ‡å—ã€‹çš„"ç»Ÿè®¡åˆ†æ"ç« èŠ‚

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import pandas as pd
import numpy as np

def main():
    print("=== Pandas ç»Ÿè®¡åˆ†æå®Œå…¨æ¼”ç¤º ===\n")

    # åˆ›å»ºå‘˜å·¥æ•°æ®
    df = pd.DataFrame({
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ', 'å­™å…«'],
        'å¹´é¾„': [25, 30, 35, 28, 32, 27],
        'éƒ¨é—¨': ['æŠ€æœ¯', 'é”€å”®', 'æŠ€æœ¯', 'å¸‚åœº', 'æŠ€æœ¯', 'é”€å”®'],
        'å·¥èµ„': [8000, 12000, 15000, 10000, 13000, 11000],
        'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½']
    }, index=['emp1', 'emp2', 'emp3', 'emp4', 'emp5', 'emp6'])

    print("å‘˜å·¥æ•°æ®:")
    print(df)
    print(f"\næ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"æ•°æ®ç±»å‹:\n{df.dtypes}")

    # ==================== 4.1 åŸºç¡€ç»Ÿè®¡è®¡ç®— ====================
    print("\n" + "="*60)
    print("4.1 åŸºç¡€ç»Ÿè®¡è®¡ç®—")
    print("="*60)

    # å•åˆ—ç»Ÿè®¡
    print("\n--- å•åˆ—ç»Ÿè®¡ ---")

    print("1. å¹´é¾„ç»Ÿè®¡:")
    age_stats = df['å¹´é¾„'].describe()
    print(age_stats)

    print("\n2. å·¥èµ„ç»Ÿè®¡:")
    salary_stats = df['å·¥èµ„'].describe()
    print(salary_stats)

    # æ‰‹åŠ¨è®¡ç®—å„é¡¹ç»Ÿè®¡æŒ‡æ ‡
    print("\n--- æ‰‹åŠ¨è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ ---")

    age_series = df['å¹´é¾„']
    print("å¹´é¾„ - æ‰‹åŠ¨ç»Ÿè®¡:")
    print(f"  è®¡æ•°: {age_series.count()}")
    print(f"  å‡å€¼: {age_series.mean():.2f}")
    print(f"  ä¸­ä½æ•°: {age_series.median():.2f}")
    print(f"  æ ‡å‡†å·®: {age_series.std():.2f}")
    print(f"  æ–¹å·®: {age_series.var():.2f}")
    print(f"  æœ€å°å€¼: {age_series.min()}")
    print(f"  æœ€å¤§å€¼: {age_series.max()}")
    print(f"  æå·®: {age_series.max() - age_series.min()}")
    print(f"  25%åˆ†ä½æ•°: {age_series.quantile(0.25):.2f}")
    print(f"  75%åˆ†ä½æ•°: {age_series.quantile(0.75):.2f}")
    print(f"  å››åˆ†ä½è·: {age_series.quantile(0.75) - age_series.quantile(0.25):.2f}")

    # è‡ªå®šä¹‰ç»Ÿè®¡å‡½æ•°
    print("\n--- è‡ªå®šä¹‰ç»Ÿè®¡å‡½æ•° ---")

    def custom_stats(series, name="æ•°æ®"):
        """è‡ªå®šä¹‰ç»Ÿè®¡å‡½æ•°"""
        stats_dict = {
            'è®¡æ•°': series.count(),
            'æ€»å’Œ': series.sum(),
            'å‡å€¼': series.mean(),
            'ä¸­ä½æ•°': series.median(),
            'ä¼—æ•°': series.mode().iloc[0] if not series.mode().empty else np.nan,
            'æ ‡å‡†å·®': series.std(),
            'æ–¹å·®': series.var(),
            'æœ€å°å€¼': series.min(),
            'æœ€å¤§å€¼': series.max(),
            'æå·®': series.max() - series.min(),
            'å››åˆ†ä½è·': series.quantile(0.75) - series.quantile(0.25),
            'å˜å¼‚ç³»æ•°': series.std() / series.mean() if series.mean() != 0 else 0,
            'ååº¦': series.skew(),
            'å³°åº¦': series.kurtosis()
        }
        return pd.Series(stats_dict, name=name)

    print("å¹´é¾„è‡ªå®šä¹‰ç»Ÿè®¡:")
    print(custom_stats(df['å¹´é¾„'], "å¹´é¾„"))
    print("\nå·¥èµ„è‡ªå®šä¹‰ç»Ÿè®¡:")
    print(custom_stats(df['å·¥èµ„'], "å·¥èµ„"))

    # å¤šåˆ—åŒæ—¶ç»Ÿè®¡
    print("\n--- å¤šåˆ—åŒæ—¶ç»Ÿè®¡ ---")

    # æ•°å€¼åˆ—ç»Ÿè®¡
    numeric_cols = df.select_dtypes(include=[np.number])
    print("æ‰€æœ‰æ•°å€¼åˆ—ç»Ÿè®¡:")
    print(numeric_cols.describe())

    # è‡ªå®šä¹‰å¤šåˆ—ç»Ÿè®¡
    print("\nå¤šåˆ—è‡ªå®šä¹‰ç»Ÿè®¡:")
    multi_stats = pd.DataFrame({
        'å¹´é¾„': custom_stats(df['å¹´é¾„']),
        'å·¥èµ„': custom_stats(df['å·¥èµ„'])
    })
    print(multi_stats.round(2))

    # ==================== 4.2 åˆ†ç»„ç»Ÿè®¡ ====================
    print("\n" + "="*60)
    print("4.2 åˆ†ç»„ç»Ÿè®¡")
    print("="*60)

    # æŒ‰éƒ¨é—¨åˆ†ç»„ç»Ÿè®¡
    print("\n--- æŒ‰éƒ¨é—¨åˆ†ç»„ç»Ÿè®¡ ---")

    dept_stats = df.groupby('éƒ¨é—¨').agg({
        'å¹´é¾„': ['count', 'mean', 'std', 'min', 'max'],
        'å·¥èµ„': ['mean', 'median', 'min', 'max', 'std']
    }).round(2)

    print("æŒ‰éƒ¨é—¨ç»Ÿè®¡çš„å¹´é¾„å’Œå·¥èµ„:")
    print(dept_stats)

    # æ‰å¹³åŒ–å¤šçº§åˆ—å
    print("\næ‰å¹³åŒ–åˆ—å:")
    dept_stats.columns = ['_'.join(col).strip() for col in dept_stats.columns]
    print(dept_stats)

    # æŒ‰åŸå¸‚åˆ†ç»„ç»Ÿè®¡
    print("\n--- æŒ‰åŸå¸‚åˆ†ç»„ç»Ÿè®¡ ---")

    city_stats = df.groupby('åŸå¸‚').agg({
        'å§“å': 'count',
        'å¹´é¾„': ['mean', 'min', 'max'],
        'å·¥èµ„': ['mean', 'sum']
    }).round(2)

    city_stats.columns = ['å‘˜å·¥æ•°', 'å¹³å‡å¹´é¾„', 'æœ€å°å¹´é¾„', 'æœ€å¤§å¹´é¾„', 'å¹³å‡å·¥èµ„', 'æ€»å·¥èµ„']
    print(city_stats)

    # å¤šçº§åˆ†ç»„ç»Ÿè®¡
    print("\n--- å¤šçº§åˆ†ç»„ç»Ÿè®¡ ---")

    # æ·»åŠ å¹´é¾„æ®µåˆ—
    df['å¹´é¾„æ®µ'] = pd.cut(df['å¹´é¾„'], bins=[20, 25, 30, 35, 40], labels=['20-25', '25-30', '30-35', '35-40'])

    multi_group = df.groupby(['éƒ¨é—¨', 'å¹´é¾„æ®µ']).agg({
        'å§“å': 'count',
        'å·¥èµ„': ['mean', 'median']
    }).round(2)

    multi_group.columns = ['å‘˜å·¥æ•°', 'å¹³å‡å·¥èµ„', 'å·¥èµ„ä¸­ä½æ•°']
    print("éƒ¨é—¨ + å¹´é¾„æ®µåˆ†ç»„ç»Ÿè®¡:")
    print(multi_group)

    # é€è§†è¡¨ç»Ÿè®¡
    print("\n--- é€è§†è¡¨ç»Ÿè®¡ ---")

    pivot_table = pd.pivot_table(df,
                                values='å·¥èµ„',
                                index='éƒ¨é—¨',
                                columns='åŸå¸‚',
                                aggfunc='mean',
                                fill_value=0,
                                margins=True,
                                margins_name='æ€»è®¡')

    print("éƒ¨é—¨-åŸå¸‚å·¥èµ„é€è§†è¡¨:")
    print(pivot_table.round(2))

    # äº¤å‰è¡¨
    print("\n--- äº¤å‰è¡¨ç»Ÿè®¡ ---")

    cross_tab = pd.crosstab(df['éƒ¨é—¨'], df['å¹´é¾„æ®µ'], margins=True)
    print("éƒ¨é—¨-å¹´é¾„æ®µäº¤å‰è¡¨:")
    print(cross_tab)

    # é«˜çº§åˆ†ç»„æ“ä½œ
    print("\n--- é«˜çº§åˆ†ç»„æ“ä½œ ---")

    # è‡ªå®šä¹‰èšåˆå‡½æ•°
    def salary_range(series):
        return f"{series.min()}-{series.max()}"

    custom_group = df.groupby('éƒ¨é—¨').agg({
        'å·¥èµ„': [salary_range, 'mean', 'std'],
        'å¹´é¾„': ['count', lambda x: x.mean().round(1)]
    })
    custom_group.columns = ['å·¥èµ„èŒƒå›´', 'å¹³å‡å·¥èµ„', 'å·¥èµ„æ ‡å‡†å·®', 'å‘˜å·¥æ•°', 'å¹³å‡å¹´é¾„']
    print("è‡ªå®šä¹‰èšåˆå‡½æ•°ç»“æœ:")
    print(custom_group)

    # ==================== 4.3 é«˜çº§ç»Ÿè®¡åˆ†æ ====================
    print("\n" + "="*60)
    print("4.3 é«˜çº§ç»Ÿè®¡åˆ†æ")
    print("="*60)

    # åˆ›å»ºæ›´å¤šç¤ºä¾‹æ•°æ®
    print("\nåˆ›å»ºæ‰©å±•æ•°æ®é›†...")
    np.random.seed(42)
    extended_df = pd.DataFrame({
        'äº§å“ç±»åˆ«': np.random.choice(['A', 'B', 'C'], 100),
        'é”€å”®é¢': np.random.normal(1000, 200, 100),
        'æˆæœ¬': np.random.normal(600, 100, 100),
        'å®¢æˆ·æ»¡æ„åº¦': np.random.uniform(3.0, 5.0, 100),
        'å‘˜å·¥ID': np.random.choice(['E001', 'E002', 'E003', 'E004'], 100)
    })

    # è®¡ç®—åˆ©æ¶¦ç‡
    extended_df['åˆ©æ¶¦ç‡'] = (extended_df['é”€å”®é¢'] - extended_df['æˆæœ¬']) / extended_df['é”€å”®é¢'] * 100
    extended_df['åˆ©æ¶¦'] = extended_df['é”€å”®é¢'] - extended_df['æˆæœ¬']

    print("æ‰©å±•æ•°æ®æ ·æœ¬ (å‰10è¡Œ):")
    print(extended_df.head(10))

    # æŒ‰äº§å“ç±»åˆ«çš„è¯¦ç»†ç»Ÿè®¡
    print("\n--- æŒ‰äº§å“ç±»åˆ«çš„è¯¦ç»†ç»Ÿè®¡ ---")

    detailed_stats = extended_df.groupby('äº§å“ç±»åˆ«').agg({
        'é”€å”®é¢': ['count', 'mean', 'std', 'min', 'max'],
        'æˆæœ¬': ['mean', 'std'],
        'åˆ©æ¶¦ç‡': ['mean', 'std'],
        'å®¢æˆ·æ»¡æ„åº¦': ['mean', 'std']
    }).round(2)

    print("äº§å“ç±»åˆ«è¯¦ç»†ç»Ÿè®¡:")
    print(detailed_stats)

    # ç›¸å…³æ€§åˆ†æ
    print("\n--- ç›¸å…³æ€§åˆ†æ ---")

    numeric_columns = ['é”€å”®é¢', 'æˆæœ¬', 'åˆ©æ¶¦ç‡', 'å®¢æˆ·æ»¡æ„åº¦', 'åˆ©æ¶¦']
    correlation_matrix = extended_df[numeric_columns].corr()

    print("æ•°å€¼å˜é‡ç›¸å…³ç³»æ•°çŸ©é˜µ:")
    print(correlation_matrix.round(3))

    # æ‰¾å‡ºæœ€å¼ºçš„ç›¸å…³å…³ç³»
    print("\næœ€å¼ºç›¸å…³å…³ç³»:")
    max_corr = 0
    max_pair = ('', '')
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > abs(max_corr):
                max_corr = corr_val
                max_pair = (correlation_matrix.columns[i], correlation_matrix.columns[j])

    print(f"æœ€é«˜ç›¸å…³æ€§: {max_pair[0]} å’Œ {max_pair[1]}, ç›¸å…³ç³»æ•°: {max_corr:.3f}")

    # ç½®ä¿¡åŒºé—´è®¡ç®—
    print("\n--- ç½®ä¿¡åŒºé—´è®¡ç®— ---")

    def confidence_interval(series, confidence=0.95):
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        try:
            from scipy import stats
            n = len(series)
            mean = series.mean()
            std_err = stats.sem(series)
            h = std_err * stats.t.ppf((1 + confidence) / 2, n-1)
            return (mean - h, mean + h)
        except ImportError:
            # å¦‚æœæ²¡æœ‰scipyï¼Œä½¿ç”¨æ­£æ€åˆ†å¸ƒè¿‘ä¼¼
            n = len(series)
            mean = series.mean()
            std_err = series.std() / np.sqrt(n)
            h = std_err * 1.96  # 95%ç½®ä¿¡åŒºé—´å¯¹åº”çš„Zå€¼
            return (mean - h, mean + h)

    print("95%ç½®ä¿¡åŒºé—´è®¡ç®—:")
    for category in extended_df['äº§å“ç±»åˆ«'].unique():
        sales_data = extended_df[extended_df['äº§å“ç±»åˆ«'] == category]['é”€å”®é¢']
        ci_lower, ci_upper = confidence_interval(sales_data)
        print(f"{category} é”€å”®é¢ 95% ç½®ä¿¡åŒºé—´: ({ci_lower:.2f}, {ci_upper:.2f})")

    # å¼‚å¸¸å€¼æ£€æµ‹
    print("\n--- å¼‚å¸¸å€¼æ£€æµ‹ ---")

    def detect_outliers_iqr(series):
        """ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = series[(series < lower_bound) | (series > upper_bound)]
        return outliers, lower_bound, upper_bound

    def detect_outliers_zscore(series, threshold=3):
        """ä½¿ç”¨Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        z_scores = np.abs((series - series.mean()) / series.std())
        outliers = series[z_scores > threshold]
        return outliers

    print("å¼‚å¸¸å€¼æ£€æµ‹ (IQR æ–¹æ³•):")
    for category in extended_df['äº§å“ç±»åˆ«'].unique():
        category_data = extended_df[extended_df['äº§å“ç±»åˆ«'] == category]
        outliers, lower, upper = detect_outliers_iqr(category_data['é”€å”®é¢'])
        print(f"{category} é”€å”®é¢:")
        print(f"  æ­£å¸¸èŒƒå›´: ({lower:.2f}, {upper:.2f})")
        print(f"  å¼‚å¸¸å€¼æ•°é‡: {len(outliers)}")
        if len(outliers) > 0:
            print(f"  å¼‚å¸¸å€¼: {outliers.values}")

    print("\nå¼‚å¸¸å€¼æ£€æµ‹ (Z-score æ–¹æ³•):")
    for category in extended_df['äº§å“ç±»åˆ«'].unique():
        category_data = extended_df[extended_df['äº§å“ç±»åˆ«'] == category]
        outliers = detect_outliers_zscore(category_data['é”€å”®é¢'])
        print(f"{category} é”€å”®é¢å¼‚å¸¸å€¼ (Z-score > 3): {len(outliers)} ä¸ª")

    # åˆ†å¸ƒç»Ÿè®¡
    print("\n--- åˆ†å¸ƒç»Ÿè®¡ ---")

    def distribution_stats(series, name="æ•°æ®"):
        """è®¡ç®—åˆ†å¸ƒç»Ÿè®¡"""
        stats_dict = {
            'å‡å€¼': series.mean(),
            'ä¸­ä½æ•°': series.median(),
            'ä¼—æ•°': series.mode().iloc[0] if not series.mode().empty else np.nan,
            'æ ‡å‡†å·®': series.std(),
            'ååº¦': series.skew(),
            'å³°åº¦': series.kurtosis(),
            'å˜å¼‚ç³»æ•°': series.std() / series.mean() if series.mean() != 0 else 0,
            'èŒƒå›´': series.max() - series.min(),
            'å››åˆ†ä½è·': series.quantile(0.75) - series.quantile(0.25)
        }

        # åˆ†å¸ƒç±»å‹åˆ¤æ–­
        skewness = stats_dict['ååº¦']
        if abs(skewness) < 0.5:
            distribution = "è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ"
        elif skewness > 0.5:
            distribution = "å³ååˆ†å¸ƒ"
        else:
            distribution = "å·¦ååˆ†å¸ƒ"

        stats_dict['åˆ†å¸ƒç±»å‹'] = distribution
        return pd.Series(stats_dict, name=name)

    for category in extended_df['äº§å“ç±»åˆ«'].unique():
        category_sales = extended_df[extended_df['äº§å“ç±»åˆ«'] == category]['é”€å”®é¢']
        print(f"\n{category} é”€å”®é¢åˆ†å¸ƒç»Ÿè®¡:")
        print(distribution_stats(category_sales, f"{category}é”€å”®é¢").round(3))

    # å‘˜å·¥æ•ˆç‡åˆ†æ
    print("\n" + "="*60)
    print("å‘˜å·¥æ•ˆç‡åˆ†æ")
    print("="*60)

    # å‘˜å·¥ç»©æ•ˆç»Ÿè®¡
    print("\n--- å‘˜å·¥ç»©æ•ˆç»Ÿè®¡ ---")

    # è®¡ç®—å·¥èµ„æ•ˆç‡æŒ‡æ ‡
    df['å·¥èµ„ç­‰çº§'] = pd.cut(df['å·¥èµ„'],
                           bins=[0, 8000, 12000, 16000],
                           labels=['åˆçº§', 'ä¸­çº§', 'é«˜çº§'])

    performance_stats = df.groupby(['éƒ¨é—¨', 'å·¥èµ„ç­‰çº§']).agg({
        'å¹´é¾„': ['count', 'mean'],
        'å·¥èµ„': ['mean', 'std']
    }).round(2)

    performance_stats.columns = ['å‘˜å·¥æ•°', 'å¹³å‡å¹´é¾„', 'å¹³å‡å·¥èµ„', 'å·¥èµ„æ ‡å‡†å·®']
    print("éƒ¨é—¨-å·¥èµ„ç­‰çº§ç»©æ•ˆç»Ÿè®¡:")
    print(performance_stats)

    # å·¥èµ„åˆ†å¸ƒåˆ†æ
    print("\n--- å·¥èµ„åˆ†å¸ƒåˆ†æ ---")

    print("æ•´ä½“å·¥èµ„åˆ†å¸ƒ:")
    salary_dist = distribution_stats(df['å·¥èµ„'], "å·¥èµ„")
    print(salary_dist.round(2))

    print("\nå„éƒ¨é—¨å·¥èµ„åˆ†å¸ƒ:")
    for dept in df['éƒ¨é—¨'].unique():
        dept_salary = df[df['éƒ¨é—¨'] == dept]['å·¥èµ„']
        dept_stats = distribution_stats(dept_salary, f"{dept}éƒ¨é—¨å·¥èµ„")
        print(dept_stats.round(2))

    # å¹´é¾„ä¸å·¥èµ„å…³ç³»åˆ†æ
    print("\n--- å¹´é¾„ä¸å·¥èµ„å…³ç³»åˆ†æ ---")

    # è®¡ç®—ç›¸å…³ç³»æ•°
    age_salary_corr = df['å¹´é¾„'].corr(df['å·¥èµ„'])
    print(f"å¹´é¾„ä¸å·¥èµ„ç›¸å…³ç³»æ•°: {age_salary_corr:.3f}")

    # æŒ‰å¹´é¾„æ®µåˆ†æå·¥èµ„
    age_groups = df.groupby('å¹´é¾„æ®µ').agg({
        'å·¥èµ„': ['count', 'mean', 'std', 'min', 'max'],
        'å¹´é¾„': ['mean', 'min', 'max']
    }).round(2)
    age_groups.columns = ['äººæ•°', 'å¹³å‡å·¥èµ„', 'å·¥èµ„æ ‡å‡†å·®', 'æœ€ä½å·¥èµ„', 'æœ€é«˜å·¥èµ„', 'å¹³å‡å¹´é¾„', 'æœ€å°å¹´é¾„', 'æœ€å¤§å¹´é¾„']
    print("\nå¹´é¾„æ®µå·¥èµ„åˆ†æ:")
    print(age_groups)

    # ==================== å®ç”¨ç»Ÿè®¡åˆ†æå·¥å…· ====================
    print("\n" + "="*60)
    print("å®ç”¨ç»Ÿè®¡åˆ†æå·¥å…·")
    print("="*60)

    # è‡ªåŠ¨åŒ–ç»Ÿè®¡æŠ¥å‘Šå‡½æ•°
    def generate_stats_report(dataframe, group_col=None, value_cols=None):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if value_cols is None:
            value_cols = dataframe.select_dtypes(include=[np.number]).columns.tolist()

        report = {}

        if group_col:
            # åˆ†ç»„ç»Ÿè®¡
            for group in dataframe[group_col].unique():
                group_data = dataframe[dataframe[group_col] == group]
                report[group] = {}
                for col in value_cols:
                    if col in group_data.columns:
                        report[group][col] = distribution_stats(group_data[col], col)
        else:
            # æ•´ä½“ç»Ÿè®¡
            for col in value_cols:
                if col in dataframe.columns:
                    report[col] = distribution_stats(dataframe[col], col)

        return report

    # ç”Ÿæˆå‘˜å·¥ç»Ÿè®¡æŠ¥å‘Š
    print("\n--- è‡ªåŠ¨åŒ–ç»Ÿè®¡æŠ¥å‘Š ---")
    employee_report = generate_stats_report(df, group_col='éƒ¨é—¨', value_cols=['å¹´é¾„', 'å·¥èµ„'])

    for dept, stats in employee_report.items():
        print(f"\n{dept}éƒ¨é—¨ç»Ÿè®¡:")
        for metric, values in stats.items():
            print(f"  {metric}: å‡å€¼={values['å‡å€¼']:.2f}, æ ‡å‡†å·®={values['æ ‡å‡†å·®']:.2f}, åˆ†å¸ƒ={values['åˆ†å¸ƒç±»å‹']}")

    # æ•°æ®è´¨é‡è¯„ä¼°
    print("\n--- æ•°æ®è´¨é‡è¯„ä¼° ---")

    def data_quality_assessment(dataframe):
        """æ•°æ®è´¨é‡è¯„ä¼°"""
        quality_report = {
            'æ€»è¡Œæ•°': len(dataframe),
            'æ€»åˆ—æ•°': len(dataframe.columns),
            'ç¼ºå¤±å€¼': dataframe.isnull().sum().sum(),
            'é‡å¤è¡Œ': dataframe.duplicated().sum(),
            'æ•°å€¼åˆ—æ•°': len(dataframe.select_dtypes(include=[np.number]).columns),
            'æ–‡æœ¬åˆ—æ•°': len(dataframe.select_dtypes(include=['object']).columns),
            'æ—¥æœŸåˆ—æ•°': len(dataframe.select_dtypes(include=['datetime64']).columns)
        }

        # è®¡ç®—æ¯åˆ—çš„æ•°æ®è´¨é‡
        column_quality = {}
        for col in dataframe.columns:
            col_quality = {
                'ç¼ºå¤±å€¼æ•°': dataframe[col].isnull().sum(),
                'ç¼ºå¤±å€¼æ¯”ä¾‹': dataframe[col].isnull().sum() / len(dataframe) * 100,
                'å”¯ä¸€å€¼æ•°': dataframe[col].nunique(),
                'æ•°æ®ç±»å‹': str(dataframe[col].dtype)
            }
            column_quality[col] = col_quality

        quality_report['åˆ—è´¨é‡'] = column_quality
        return quality_report

    quality_report = data_quality_assessment(df)
    print("æ•°æ®è´¨é‡æŠ¥å‘Š:")
    for key, value in quality_report.items():
        if key != 'åˆ—è´¨é‡':
            print(f"  {key}: {value}")

    print("\nåˆ—è´¨é‡è¯¦æƒ…:")
    for col, quality in quality_report['åˆ—è´¨é‡'].items():
        print(f"  {col}: ç±»å‹={quality['æ•°æ®ç±»å‹']}, ç¼ºå¤±å€¼={quality['ç¼ºå¤±å€¼æ•°']}({quality['ç¼ºå¤±å€¼æ¯”ä¾‹']:.1f}%), å”¯ä¸€å€¼={quality['å”¯ä¸€å€¼æ•°']}")

    print("\n" + "="*60)
    print("ç»Ÿè®¡åˆ†ææ¼”ç¤ºå®Œæˆ!")
    print("="*60)

    # æ€»ç»“
    print("\nã€ç»Ÿè®¡åˆ†ææ€»ç»“ã€‘")
    print("âœ“ åŸºç¡€ç»Ÿè®¡: describe, è‡ªå®šä¹‰ç»Ÿè®¡å‡½æ•°")
    print("âœ“ åˆ†ç»„ç»Ÿè®¡: groupby, é€è§†è¡¨, äº¤å‰è¡¨")
    print("âœ“ é«˜çº§åˆ†æ: ç›¸å…³æ€§, ç½®ä¿¡åŒºé—´, å¼‚å¸¸å€¼æ£€æµ‹")
    print("âœ“ åˆ†å¸ƒåˆ†æ: ååº¦, å³°åº¦, åˆ†å¸ƒç±»å‹åˆ¤æ–­")
    print("âœ“ å®ç”¨å·¥å…·: è‡ªåŠ¨åŒ–æŠ¥å‘Š, æ•°æ®è´¨é‡è¯„ä¼°")
    print("âœ“ å¯è§†åŒ–å‡†å¤‡: ä¸ºæ•°æ®å¯è§†åŒ–å‡†å¤‡ç»Ÿè®¡æ•°æ®")

    print(f"\nåŸå§‹å‘˜å·¥æ•°æ®: {df.shape}")
    print(f"æ‰©å±•äº§å“æ•°æ®: {extended_df.shape}")
    print("æ‰€æœ‰ç»Ÿè®¡åˆ†ææ¼”ç¤ºå‡æˆåŠŸå®Œæˆ! ğŸ“Š")

if __name__ == "__main__":
    main()